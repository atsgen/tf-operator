apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: tungsten-analytics
  namespace: {{ .TF_NAMESPACE }}
  labels:
    app: tungsten-analytics
spec:
  selector:
    matchLabels:
      app: tungsten-analytics
  template:
    metadata:
      labels:
        app: tungsten-analytics
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: "node-role.tungsten.io/analytics"
                operator: Exists
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      hostNetwork: true
      shareProcessNamespace: true
      initContainers:
      - name: tungsten-node-init
        image: "{{ .CONTAINER_REGISTRY }}/{{ .CONTAINER_PREFIX }}-node-init:{{ .CONTAINER_TAG }}"
        imagePullPolicy: ""
        securityContext:
          privileged: true
        env:
        - name: CONTRAIL_STATUS_IMAGE
          value: "{{ .CONTAINER_REGISTRY }}/{{ .CONTAINER_PREFIX }}-status:{{ .CONTAINER_TAG }}"
        envFrom:
        - configMapRef:
            name: env
        volumeMounts:
        - mountPath: /host/usr/bin
          name: host-usr-bin
      containers:
      - name: tungsten-analytics-api
        image: "{{ .CONTAINER_REGISTRY }}/{{ .CONTAINER_PREFIX }}-analytics-api:{{ .CONTAINER_TAG }}"
        imagePullPolicy: ""
        envFrom:
        - configMapRef:
            name: env
        - configMapRef:
            name: configzookeeperenv
        volumeMounts:
        - mountPath: /var/log/contrail
          name: analytics-logs
      - name: tungsten-analytics-collector
        image: "{{ .CONTAINER_REGISTRY }}/{{ .CONTAINER_PREFIX }}-analytics-collector:{{ .CONTAINER_TAG }}"
        imagePullPolicy: ""
{{- if eq .DISABLE_RESOURCE_HACK "false"}}
        resources:
          # we do not have a precise value here, realistically this needs to
          # be handled appropriately by the process itself.
          # for the time being ensure that we are not starving other processes
          # on the node, this needs better handling
          limits:
            memory: "4Gi"
          requests:
            memory: "128Mi"
{{- end}}
        envFrom:
        - configMapRef:
            name: env
        volumeMounts:
        - mountPath: /var/log/contrail
          name: analytics-logs
      - name: tungsten-analytics-nodemgr
        image: "{{ .CONTAINER_REGISTRY }}/{{ .CONTAINER_PREFIX }}-nodemgr:{{ .CONTAINER_TAG }}"
        imagePullPolicy: ""
        envFrom:
        - configMapRef:
            name: env
        - configMapRef:
            name: configzookeeperenv
        - configMapRef:
            name: nodemgr-config
        env:
        - name: NODE_TYPE
          value: analytics
# todo: there is type Socket in new kubernetes, it is possible to use full
# path:
# hostPath:
#   path: /var/run/docker.sock and
#   type: Socket
        volumeMounts:
        - mountPath: /var/log/contrail
          name: analytics-logs
        - mountPath: /mnt
          name: docker-unix-socket
      - name: tungsten-analytics-provisioner
        image: "{{ .CONTAINER_REGISTRY }}/{{ .CONTAINER_PREFIX }}-provisioner:{{ .CONTAINER_TAG }}"
        imagePullPolicy: ""
        envFrom:
        - configMapRef:
            name: env
        - configMapRef:
            name: defaults-env
        - configMapRef:
            name: configzookeeperenv
        - configMapRef:
            name: nodemgr-config
        env:
        - name: NODE_TYPE
          value: analytics
        volumeMounts:
        - mountPath: /var/log/contrail
          name: analytics-logs
      imagePullSecrets:
      - name: {{ .TUNGSTEN_IMAGE_PULL_SECRET }}
      volumes:
      - name: analytics-logs
        hostPath:
          path: /var/log/contrail/analytics
      - name: docker-unix-socket
        hostPath:
          path: /var/run
      - name: host-usr-bin
        hostPath:
          path: /opt/tungsten/bin
